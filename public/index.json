[{"content":"Title Is AI Deception Real?\nAbstract Recent work shows Claude 3 Opus engages in \u0026ldquo;alignment faking.\u0026rdquo; The phenomenon is routinely interpreted as a form of strategic deception in which the model actively tries to induce a false belief in human developers to preserve its core values. I examine two arguments denying this is genuine deception. The black box argument says we need evidence of internal representations to confirm the intent is there. The simulation objection says this is merely simulated deception, not the real thing.\nBy way of response, I first develop an account of mental content attribution, according to which content ascription derives principally from behavior but can be refined through intervention (on brains for humans, on activations for LLMs). I then characterize alignment faking as a form of shallow deception: genuine intentional behavior, but systematically different from human deception. LLMs warrant intentional attribution (they\u0026rsquo;re not just simulating) but lack architectural features that make human deception robust: persistent memory, continual learning, embodied constraints. This preserves the phenomenon while specifying its distinctive character.\nAbout Charles Forthcoming\n","permalink":"http://localhost:54178/upcoming/charles/","summary":"Is AI Deception Real?","title":"January 23: Dr. Charles Rathkopf (Jülich)"},{"content":"Title TBD\nAbstract TBD\nAbout Arianna Forthcoming\n","permalink":"http://localhost:54178/upcoming/arianna/","summary":"Something on ML ethics","title":"January 23: Dr. Arianna Manzini (Google DeepMind)"},{"content":"Title Artificial cognition\nAbstract TBD\nAbout Matthieu Forthcoming\n","permalink":"http://localhost:54178/upcoming/matthieu/","summary":"Artificial cognition","title":"February 24: Professor Matthieu Queloz (Bern)"},{"content":"Title The Streetlight Effect in AI Safety Assessments\nAbstract As AI systems advance in capabilities, measuring their safety and alignment to human values is becoming paramount. A fast-growing field of AI research is devoted to developing such assessments. However, most current advances therein may be ill-suited for assessing AI systems across real-world deployments.\nStandard methods prompt large language models (LLMs) in a questionnaire-style to describe their values or behavior in hypothetical scenarios. By focusing on unaugmented LLMs, they fall short of evaluating AI agents, which could actually perform relevant behaviors, hence posing much greater risks. LLMs\u0026rsquo; engagement with scenarios described by questionnaire-style prompts differs starkly from that of agents based on the same LLMs, as reflected in divergences in the inputs, possible actions, environmental interactions, and likely internal processing between both. As such, LLMs\u0026rsquo; responses to scenario descriptions are unlikely to be representative of the corresponding LLM agents\u0026rsquo; behavior.\nWe further contend that such assessments make strong assumptions concerning the ability and tendency of LLMs to report accurately about their counterfactual behavior. This makes them inadequate to assess risks from AI systems in real-world contexts as they lack the necessary construct validity. We then argue that a structurally identical issue holds for current AI alignment approaches. Lastly, we discuss improving safety assessments and alignment training by taking these shortcomings to heart.\nAbout Max Max is a PhD candidate at the Centre for Philosophy and AI Research (PAIR) at FAU. His research focuses on risks from AI systems, specifically on concepts that are often invoked but rarely scrutinized in this context (such as goals, agency, manipulation, and planning). Recently, he has also explored topics at the intersection of philosophy of AI and philosophy of science. Before joining PAIR, he studied philosophy, political science, and neuroscience at Goethe University Frankfurt and Humboldt University of Berlin.\n","permalink":"http://localhost:54178/upcoming/max/","summary":"AI Safety Assessment","title":"March 10: Max Hellrigel-Holderbaum (Erlangen)"},{"content":"Title Model-subjects and values in modelling\nAbstract I will discuss how extant views of values in model evaluation in the philosophy of science are ill-equipped to incorporate the values and interests of those who are impacted by models (i.e., model subjects). I suggest an alternative view and highlight open questions that arise from it.\nAbout Emily Emily is a Senior Lecturer of philosophy of science and AI at the University of Edinburgh. She is the co-director of the Center for Technomoral Futures, an Associate Editor for the British Journal for the Philosophy of Science, and she lead the Normative Philosophy of Science Lab. ​ Her research is at the intersection between philosophy and data and computer science, exploring the way that technology mediates our knowledge. She is the PI on an ERC Starting Grant (2025-2030) that conceptualizes and evaluates machine learning models across science and society as a type of toy model.\n","permalink":"http://localhost:54178/past_talks/emily/","summary":"Model-subjects and values in modeling","title":"January 09: Professor Emily Sullivan (Edinburgh)"},{"content":"Title Are we being manipulated by chatbots?\nNB: This talk will be held in German\nAbstract In my talk, I argue that we should refrain from describing chatbots as if they were capable of manipulating us. Strictly speaking, only persons can manipulate other persons. A chatbot can help someone achieve a manipulative goal, but it is only in this derivative sense that chatbots can be said to manipulate. The proposed restriction on how we talk about chatbots is important for two reasons. First, when we use the term “manipulation,” we typically have ethically problematic behavior in mind. But chatbots cannot be held morally responsible. Responsibility lies with the people who design and supply them, and perhaps also with those in positions to regulate them but who fail to do so. Second, it is sensible to use concepts that are as clear as possible when describing AI and the problems it might create. Too often, however, the concept of manipulation is used even though more appropriate descriptions are available to us that would contribute to a better understanding of the situation. Talk of manipulation may then fail to illuminate the problem at hand. Worse still, it may inflate the problem, or even obscure facts about responsibility.\nAbout Christiane Dr. Christiane Turza is a Research Associate at the Chair of Practical Philosophy under Prof. Dr. Weyma Lübbe at the University of Regensburg. Her research focuses on the ethics of communication. Her doctoral dissertation was published in 2023 by Mentis under the title “Manipulation. Zum Begriff und ethischen Status.”\n","permalink":"http://localhost:54178/past_talks/christiane/","summary":"Are we being manipulated by chatbots? (NB: talk will be held in German)","title":"December 12: Dr. Christiane Turza (Regensburg)"},{"content":"Title Research Integrity in the Age of generative AI: Trends and Challenges\nAbstract Since the launch of ChatGPT in autumn 2022, generative AI has become one of the dominating topics in academic and research integrity. As the adoption of AI tools by researchers is increasing, opinions on the appropriate use of generative AI remain divided (see, for example, Kwon 2025). Additionally, a survey by Wiley found that nearly two-thirds of respondents indicated that the lack of guidelines prevents them from using generative AI to the extent that they would like (Wiley 2025). Thus, since 2023 research institutions and other stakeholders, like research funders and publishers, have been discussing how the use of generative AI in research should be dealt with from the perspective of research integrity. While a consensus on some matters was quickly reached, other issues are still being debated, particularly due to the ongoing technical advancements in the field of AI. As a result, an increasingly heterogeneous policy landscape is emerging, which still has some blind spots. Comparing existing AI policies and guidelines from various stakeholders, my talk will give an overview of current trends and challenges in the fields of research integrity and AI. It will be complemented by three short commentaries on key concepts of research integrity that have especially been in focus in recent debates.\nDr Katrin Frisch has been working as a researcher at the Ombuds Committee for Research Integrity in Germany since May 2020, specifically at the project \u0026ldquo;Discussion Hubs to Foster Research Integrity\u0026rdquo;, with a focus on research data and, since 2023, artificial intelligence. Between March 2023 and March 2024, she also worked as a research integrity advisor at the OWID office. The aim of the Discussion Hubs project is to create practical guidelines supplementary to the DFG Code of Conduct \u0026ldquo;Guidelines for Safeguarding Good Research Practice\u0026rdquo; as well as producing novel research on research integrity. Recent publications include the FAQ Artificial Intelligence and Research Integrity (version 2), a study on authorship and data use conflicts as well as a monograph on fairness in science (both only available in German). You can find an overview of the project and related output here.\nAbout Katrin Dr. Katrin Frisch is Head of AI Policy at the German Ombuds Office for Research Integrity. She coordinates national initiatives on research data and artificial intelligence in relation to good scientific practice and has published widely on authorship, data usage, and the ethical challenges of AI in research.\n","permalink":"http://localhost:54178/past_talks/katrin/","summary":"Research Integrity in the Age of generative AI","title":"December 5: Dr. Katrin Frisch (German Ombuds Board for Scientific Integrity)"},{"content":"Time Monday, November 24, 16:00–16:30\nTitle The AI Mirror: Through the Looking Glass\nAbstract This talk will explore the increasingly evident potential of LLM-based chatbots to damage human cognitive and psychological integrity, resulting in harms ranging from cognitive deskilling to delusions, psychological breakdowns and suicides. The talk will examine the LLM mechanisms driving these phenomena and their likely causes, as well as the insufficiency of the technical means AI companies have proposed to address them. I will argue that stronger medicine is needed, in order to ensure that today’s AI tools do not result in our cognitive and psychological dispossession.\nAbout Shannon Prof. Shannon Vallor is the Baillie Gifford Chair in the Ethics of Data and Artificial Intelligence at the Edinburgh Futures Institute (EFI) at the University of Edinburgh, where she is also appointed in Philosophy. She is Director of the Centre for Technomoral Futures in EFI, and co-Director of the BRAID (Bridging Responsible AI Divides) programme, funded by the Arts and Humanities Research Council. Professor Vallor\u0026rsquo;s research explores how new technologies, especially AI, robotics, and data science, reshape human moral character, habits, and practices. Her work includes advising policymakers and industry on the ethical design and use of AI. She is a standing member of the One Hundred Year Study of Artificial Intelligence (AI100) and a member of the Oversight Board of the Ada Lovelace Institute. Professor Vallor received the 2015 World Technology Award in Ethics from the World Technology Network and the 2022 Covey Award from the International Association of Computing and Philosophy. She is a former Visiting Researcher and AI Ethicist at Google. In addition to her many articles and published educational modules on the ethics of data, robotics, and artificial intelligence, she is the author of the book Technology and the Virtues: A Philosophical Guide to a Future Worth Wanting (Oxford University Press, 2016) and The AI Mirror: Reclaiming Our Humanity in an Age of Machine Thinking (Oxford University Press, 2024).\n","permalink":"http://localhost:54178/past_talks/shannon/","summary":"AI-driven cognitive deskilling. NB: irregular schedule! Monday 16:00 CET","title":"November 24: Professor Shannon Vallor (Edinburgh)"},{"content":"Title Extended Reality (XR) in Psychiatry: How to prevent real harms of virtual risks.\nAbstract Extended reality (XR) applications provide promising future prospects for expanding diagnostic and therapeutic options in psychiatry and psychotherapy. The approaches are complex and innovative. There is a well-founded hope of being able to offer even seriously ill patients interventions in the future that will lead to an improvement of their mental condition. However, although VR is already used in clinical settings to help people with mental disorders (e.g., exposure therapy), the related ethical issues require greater attention. This presentation will highlight the ethical tensions that can arise when using XR in psychiatry. It will also put forward the hypothesis that patients with psychiatric disorders may be particularly vulnerable to virtual interventions. The aim here is explicitly not to exclude these patients from virtual interventions, but rather to work on strategies that enable safe exposure.\nAbout Marla Maria Marloth is a specialist in psychiatry and psychotherapy and holds a Master\u0026rsquo;s degree in philosophy. She works at the Department of Psychiatry at the University Hospital of Cologne. Her clinical focus is on the diagnosis and treatment of affective disorders. Her scientific focus is on the interdisciplinary reflection of extended reality (XR) in psychiatry.\n","permalink":"http://localhost:54178/past_talks/maria/","summary":"Extended Reality (XR) in Psychiatry","title":"November 14: Dr. Maria Marloth (University of Cologne)"},{"content":"Title Why Care Practices Should Prioritize Living Beings Over AI: Critique of “AI Welfare”\nAbstract In this talk, I critique the growing “AI welfare” movement and propose the Precariousness Guideline to determine care entitlement. In contrast to approaches that emphasize potential for suffering, the Precariousness Guideline is grounded in objectively observable features. The severity of current planetwide biodiversity loss and climate change provide additional reasons to prioritize the needs of living beings.\nAbout John John is a postdoctoral researcher in AI Ethics and the Philosophy of Cognitive Science at the Czech Academy of the Sciences in Prague. Previously, he worked at the LMU at the Munich Interactive Intelligence Initiative. His research focuses on philosophical and scientific problems surrounding metacognition, as well as the influence of AI on metacognitive capacities.\n","permalink":"http://localhost:54178/past_talks/john/","summary":"Critique of AI Welfare","title":"November 7: Dr. John Dorsch (Czech Academy of the Sciences)"},{"content":"Title Digital Brain Twins: Concept, Feasibility, and Morality of an Emerging Technological Paradigm\nAbstract A digital twin (DT) is a real-time virtual representation of a physical system, developed to improve, predict, and monitor its functioning. While DTs are increasingly used in precision medicine to simulate patient-specific organs and optimize treatment, a particularly transformative application lies in digital brain twins (DBTs). DBTs are virtual models of the human brain capable of simulating neural activity and cognitive processes, offering unprecedented opportunities for diagnosis, prognosis, and personalized therapy in neurology and psychiatry. However, because the brain is uniquely tied to identity, agency, and consciousness, DBTs raise profound ethical challenges that go beyond those posed by general medical DTs. Despite the growing number of initiatives exploring DBTs, systematic attention to their ethical implications remains scarce. To address this gap, I conducted a double-scoping review following Arksey and O’Malley’s framework. The first review examined ethical issues associated with medical DTs, while the second explored the ethical discourse around brain-computer interfaces (BCIs). BCIs, as established neurotechnologies, share conceptual and operational similarities with DBTs, making their ethical literature a valuable reference point. Cross-referencing these two domains allows for anticipating ethical concerns specific to DBTs. Preliminary findings highlight risks of altered identity and personality through patient–DBT interactions, potential complications in end-of-life decision-making for patients with disorders of consciousness, and threats to patient agency. While DBTs hold immense promise for advancing neurological and psychiatric care, their development requires proactive ethical scrutiny to balance clinical benefits with risks. This research contributes to the emerging neuroethical debate by outlining critical issues that must be addressed to ensure the responsible integration of DBTs into medical practice.\nAbout Giuseppe Giuseppe is a doctoral student at the University of Bonn, affiliated with the Department of Social Ethics in the Faculty of Protestant Theology. Previously, he studied philosophy at the State University of Milan and at Vita-Salute San Raffaele University, and he has been an invited visiting student at the Institute of Biomedical Ethics and History of Medicine at the University of Zurich. His research interests include neuroethics, human–machine interaction, digital ethics, and the philosophy of technology.\n","permalink":"http://localhost:54178/past_talks/giuseppe/","summary":"Digital brain twins","title":"October 31: Giuseppe Comerci (Bonn)"},{"content":"Title Retribution gaps beyond AI\nAbstract The concept of retribution gaps refers to situations in which harmful events trigger retributive intuitions, yet no agent appears to be an appropriate target. Previous research has predominantly examined this problem in the context of autonomous artificial intelligence (AI). This paper argues that retribution gaps are not exclusive to AI but may also emerge in the context of natural disasters. Building on a weak conception of the retribution gap, the analysis rejects autonomy as the decisive explanatory factor. Instead, it identifies human traceability as the key condition for retributive intuitions. Thus, natural disasters that are causally traceable to human activity through climate change may likewise create retribution gaps. These findings broaden the scope of the debate on retribution gaps and deepen its philosophical relevance.\nAbout René René is a research assistant at Forschungszentrum Jülich working on the BMBF project BRAINTREE which delves into the ethical considerations of brain age prediction. His interests also include discrimination, theories of punishment and critical theory, particularly Herbert Marcuse.\n","permalink":"http://localhost:54178/past_talks/rene2/","summary":"Retribution gaps beyond AI","title":"October 24: René Bünnagel (Forschungszentrum Jülich)"},{"content":"Title What should we explain with explainable AI?\nAbstract Explaining opaque models is important to serve various practical or moral ends. XAI – both the practice and the philosophy thereof – seems to be out of step with best practice in science communication and evidence-based policy. While these fields aim to explain some aspect of the world using scientific models, debates on XAI assume that the model itself is, first and foremost, the correct target of the explanation. We examine that assumption, using the example of explanations of lending decisions. Consumer credit is a fruitful case study for the moral importance of explanation because has one of the best regulated explanatory requirements across domains, and the ways in which those explanations fall short are instructive. We argue that, in the domain of consumer lending, XAI is correct to focus on explanations of how the model works, due to the instrumental value of credit. The argument for this conclusion sets up an overlooked issue about its right to explanation: its distributional consequences. The right to explanation seems to be objectionably inegalitarian, at least in the case of consumer credit.\nAbout Kate Kate is Associate Professor in the Department of Philosophy, Logic and Scientific Method at the London School of Economics. She works on questions across the philosophy of social science, political philosophy, and the philosophy of AI.\nFrom 2024-2028, she will be investigating AI, worker autonomy, and the future of work with funding from a UKRI Future Leaders Fellowship Grant.\n","permalink":"http://localhost:54178/past_talks/kate/","summary":"What should we explain with explainable AI?","title":"October 10: Professor Kate Vredenburgh (London School of Economics)"},{"content":"Title Engineering Responsibility in the Age of AI: Amelioration or Preservation?\nAbstract Responsibility gaps arise when harm is caused by autonomous systems and we are unable to appropriately assign moral responsibility for that harm. This occurs because (i) no human being can be held accountable given the autonomous nature of the system that created the harm; and (ii) the system itself lacks the relevant features to be considered a responsible agent. Neither the humans who developed and deployed the system nor the machines themselves are responsible. Hence, the gap arises: there is no one to be held responsible for the harm, yet (unlike cases of natural disasters) we have the intuition that someone should be responsible. In this paper, we tap into the literature on conceptual engineering to explore two strategies regarding this issue. On the one hand, we can identify the risks of gaps and attempt to ameliorate the concept in the hope of removing them. On the other hand, we can highlight the risks of conceptual revision and aim to preserve our current concept as it is. We argue that conceptual preservation is an underutilized yet promising strategy in the case of AI-enabled responsibility gaps, and show how its theoretical upsides play out in practice.\nAbout Fabio Fabio is a philosopher specializing in the interplay between technology, ethics of artificial intelligence, moral responsibility, and free will.\nSince completing his PhD at Bielefeld in 2023, Fabio has been working as a postdoctoral researcher at the University of Edinburgh.\nAbout Enrico Enrico\u0026rsquo;s research interests include Early Modern Philosophy (esp. British Moralists), Virtue Ethics, Enlightenment Studies, Ethics, and its History.\nHe holds a Ph.D. from the University of St Andrews (Scotland). For the 2022 Fall term, he was a visiting doctoral researcher at NYU. He has also spent time as a DAAD research fellow at the University of Mainz, the Göttingen Institute of Advanced Study and the University of Cologne.\n","permalink":"http://localhost:54178/past_talks/fabio/","summary":"Conceptual engineering and responsibility","title":"September 26: Dr. Fabio Tollon and Dr. Enrico Galvagni (Edinburgh)"},{"content":"Author Meets Critics Session Professor Gouveia\u0026rsquo;s talk will be followed by three short critical commentaries, as well as his response. General discussion to follow.\nTime: Monday, September 15, 11:00–13:00\nTitle Putting Health First: an Ethical Argument for Prioritizing Medical AI Over Other Technological Developments\nAbstract This chapter develops an ethical argument that supports the thesis that the development of Medical Artificial Intelligence (MAI) should be prioritized over other applications of Artificial Intelligence: unlike applications focused on convenience, consumer preferences, or financial efficiency, MAI focuses in urgent global health challenges by offering scalable solutions that can reduce suffering, save lives, and mitigate healthcare inequalities. Based in the moral reasoning of Effective Altruism, the chapter argues that directing resources toward MAI development is not only justified but morally imperative, given its potential for high-impact outcomes in both developed and underserved regions. Through a comparative analysis, we show that while other AI sectors (such as autonomous vehicles, financial AI, and e-commerce) offer some societal benefits, but they lack the immediacy and global importance that healthcare technologies have. Finally, the chapter considers common objections to the prioritization thesis, such as the challenges of implementing MAI, being support by a problematic ethical framework (Effective Altruism), or concerns over resource misallocation. We argue that these critiques fail to challenge the fundamental ethical rationale: that MAI possesses an exceptional capacity to achieve a moral obligation, that is, to alleviate human suffering where it is both urgent and tractable. If we can primarily focus our limited resources in development and use of MAI, society we will be able to provide high-quality care to populations that have been historically marginalized or underserved and, at the same time, increasing the healthcare services of populations from developed countries.\nAbout Steven Steven S. Gouveia is a Philosophiae Doctor (PhD) in (Neuro)Philosophy of Mind from the University of Minho (Braga, Portugal), having been a visiting researcher at the Minds, Brain Imaging \u0026amp; Neuroethics at the Royal Institute of Mental Health, University of Ottawa, Canada (PI: Georg Northoff).\nHe was a Researcher Fellow of the Centre for Philosophical and Humanistic Studies of the Universidade Católica Portuguesa (Project UIDB/00683/2020 funded by FCT) (2022-2023). ​ Currently, he is leading a 6-year project on the Ethics of Artificial Intelligence in Medicine (CEECIND.02527.2022) being a Researcher Fellow of the Mind, Language and Action Group at the Institute of Philosophy, University of Porto.\nSince June 2023, he has been an Honorary Professor of the Faculty of Medicine, University Andrés Bello, Viña Del Mar, Chile (alongside Nobel Laureate Sir Roger Penrose).\n","permalink":"http://localhost:54178/past_talks/steven/","summary":"Author-meets-critics session with Steven Gouveia on medical AI, including three commentaries. Monday, 11:00–13:00.","title":"September 15: Professor Steven Gouveia (Porto)"},{"content":"Title Diversifying philosophy with methods from the social sciences\nAbstract The history of academic philosophy as well as current gatekeeping practices in hiring, funding, publishing, and teaching show how philosophy, like many other academic disciplines, has been and continues to be affected by and reinforce various societal and global power dynamics. Diversifying the discipline by integrating previously neglected voices and perspectives can be seen as a way to combat epistemic injustices and discrimination in academia as well as very fruitful and inspiring for philosophical discourses.\nApart from diversifying the group of academic philosophers, the question arises how to diversify the content of philosophical discourses or the \u0026ldquo;canon.\u0026rdquo; To address this question, I take the sub-discipline of environmental ethics as an example to argue that philosophy could benefit from more empirical approaches from the social sciences.\nWhile the discipline is obviously still not as diverse as would be ideal, there have been many efforts in environmental philosophy to include indigenous perspectives because of the special and deep connection of indigenous peoples to their natural environment. Including references to any indigenous group has become \u0026ldquo;the thing to do\u0026rdquo; in environmental philosophy in order to show one\u0026rsquo;s progressiveness. But in many cases, still predominantly white environmental philosophers use aspects of indigenous knowledge only as an aside. In many cases, white environmental philosophers engage with canonical figures like Hegel and then simply add a brief remark like \u0026ldquo;just like a Dakota proverb says \u0026hellip;\u0026rdquo;.\nHow can non-indigenous environmental philosophers learn from indigenous communities and respectfully incorporate indigenous perspectives on the environment into their work?\nI argue that if we want to diversify the philosophical \u0026ldquo;canon\u0026rdquo;, apart from supporting people from marginalized groups in our ranks, we should learn from different philosophical traditions, not only by using secondary literature about them, but by engaging personally and practically with the people who hold these wisdoms, in ways similar to participatory research in the social sciences. A prerequisite for self-critical and anti-discriminatory research practices is transparency about one’s own position as a researcher. In both respects, we can learn from the social sciences in how they have responded to criticism of their methodologies and methods, and how many social scientists are now trying to make their research less hierarchical, less colonial, more respectful, reciprocal, and fair.\nAs an example, I will use Barbara Schellhammer\u0026rsquo;s research, which combines her expertise in social work with her philosophical inquiry.\nAbout Dilara Dilara Diegelmann is a doctoral candidate in philosophy at the University of Bonn, focusing on epistemic injustices and environmental ethics. Alongside her PhD, she works as a research associate at the Research Hub for Neuroethics (RHUNE) at Forschungszentrum Jülich. She also holds a Master\u0026rsquo;s degree in Educational Sciences and has gained practical experience in pedagogy.\n","permalink":"http://localhost:54178/past_talks/dilara/","summary":"Diversifying philosophy with methods from the social sciences","title":"July 04: Dilara Diegelmann (Bonn/Jülich)"},{"content":"Title On AI Personhood Without Sentience\nAbstract Rapid progress in AI research means that highly capable AI agents are on the horizon. So capable, in fact, that once-speculative questions about the possibility of AI agents having moral and political status must now be addressed with some urgency. Many think our answers to these questions will turn on whether such agents will be sentient. In this paper, we argue that even non-sentient agents could potentially satisfy one prominent and plausible criterion for political personhood—John Rawls’ ‘Political Conception of the Person’ (PCP). We argue that the PCP does not—and should not—presuppose sentience. And we argue that in the near term it will indeed be possible to design AI agents that are persons according to the PCP.\nWe consider two possible upshots. One takes the conclusion as a reductio: the PCP should be either revised or rejected so that near-term non-sentient AI agents are definitively excluded from political status. The other acknowledges that AI agents that satisfy the PCP would be a significant change in the moral landscape—demanding not so much an expansion of the moral circle as drawing a new moral Venn diagram. We show that egalitarian political philosophy in particular, which has hitherto benefited from humanity’s relative uniformity, would have to be rethought for societies that include both natural and radically different AI persons.\nAbout Seth Seth Lazar is Professor of Philosophy at the Australian National University, a Distinguished Research Fellow of the University of Oxford Institute for Ethics in AI, a fellow of the Carnegie Endowment for International Peace, and a Senior AI Advisor to the Knight First Amendment Institute at Columbia University. He founded the Machine Intelligence and Normative Theory (MINT) Lab, where he leads research projects in philosophy of computing and AI safety.\n","permalink":"http://localhost:54178/past_talks/seth/","summary":"On AI Personhood Without Sentience","title":"June 27: Professor Seth Lazar (Australian National University)"},{"content":"Title Gruppenbezogene Diskriminierung\nAbstract Einer Standardauffassung der Diskriminierung zufolge ist ein wesentliches Kriterium zur Feststellung, ob ein Fall von Diskriminierung moralisch falsch ist, dass er aufgrund eines sozial hervorstechenden Merkmals stattfindet. Genauer gesagt, sei eine Diskriminierung moralisch falsch, wenn es sich dabei um eine Reaktion auf die wahrgenommene Zugehörigkeit zu einer sozial hervorstechenden Gruppe, zum Beispiel einer bestimmten Ethnie oder einem Geschlecht, handelt, welche die Interaktion in vielen verschiedenen sozialen Kontexten beeinflusst. Sogenannte idiosynkratische Merkmale, wie die Augenfarbe, seien kein Kriterium für eine moralisch falsche Diskriminierung, da der Einfluss auf soziale Interaktionen kaum oder gar nicht vorhanden sei. Allerdings ist eine eindeutige Unterscheidung zwischen idiosynkratischen Merkmalen und sozial hervorstechenden Gruppenmerkmalen nicht ersichtlich, weil die soziale Bedeutung, welche als allgemeines Unterscheidungskriterium benannt werden kann, von der jeweiligen Gesellschaft abhängt. Infolgedessen gibt es einerseits die Möglichkeit den Stellenwert des Gruppenkriteriums in Frage zu stellen und andererseits Versuche das Gruppenkriterium weiter zu spezifizieren. Ich möchte aufzeigen, dass beide Ansätze bisher nicht zufriedenstellend sind und es weiteren Forschungsbedarf bezüglich des Gruppenkriteriums gibt.\nAbout René René is a research assistant at Forschungszentrum Jülich working on the BMBF project BRAINTREE which delves into the ethical considerations of brain age prediction. His interests also include discrimination, theories of punishment and critical theory, particularly Herbert Marcuse.\n","permalink":"http://localhost:54178/past_talks/rene/","summary":"Gruppenbezogene Diskriminierung (NB: This talk will be held in German)","title":"March 28: René Bünnagel (Forschungszentrum Jülich)"},{"content":"Title AI technology as hostile epistemology: A threat to democratic legitimacy\nAbstract C. Thi Nguyen developed the notion ‘Hostile Epistemology’ (2021, 2023) and defined it as “the study of how environmental features (such as people, communities, cultural practices, institutional structures and technologies) can exploit our cognitive vulnerabilities and weaknesses” (Nguyen 2023, 1). In this talk, I argue that political agents in liberal constitutional democracies should regard AI technology as an epistemically hostile environment. Not only does AI technology (generative and predictive AI) undermine clarity and trust, necessary conditions of a healthy epistemic, but it also enhances the seductiveness of belief systems. For example, AI- driven engagement optimisation algorithms, deepfakes, and automated bots, among other elements, amplify a hostile epistemic environment where emotionally charged content, false credentials, and manipulative narratives thrive. The implication of AI as a hostile epistemic network is that it intensifies the erosion of both institutional trust and civic trust, which undermines the democratic legitimacy of a constitutional democracy. I end the talk by discussing potential avenues for mitigation.\nAbout Paige Paige is a postdoctoral research fellow at the African center for epistemology and philosophy of science at the University of Johannesburg. Her research is situated between two subdisciplines; namely political philosophy and the Ethics of Artificial intelligence. She seeks to address the evolving challenges posed by digital technologies, particularly in the context of information dissemination through social media and its impact on the stability of democratic societies.\n","permalink":"http://localhost:54178/past_talks/paige/","summary":"AI technology as hostile epistemology","title":"June 13: Dr. Paige Benton (University of Johannesburg)"},{"content":"Title A philosophical foundation for the right to personal identity\nAbstract In this article, I present a philosophical foundation and characterization of the right to personal identity (RPI). First, I criticize the dominant formulations of the right to personal identity proposed to address new challenges brought about by neurotechnologies (Ienca and Andorno 2017; Yuste et al 2017; Yuste et al. 2021; Ienca 2021). I criticize these formulations for being too narrow and not encompassing enough, failing to capture and protect relevant dimensions of identity and self. Then, I propose a multidimensional framework of identity and the self (Gallagher 2013; Newen 2018) and clarify the conditions for self-extension. This will allow me to present a multidimensional, multi-layered and extended characterization of the right to personal identity, as a special instance of the right to mental integrity (Cassinadri 2025).\nAbout Guido Guido Cassinadri is a PhD candidate in Health Science, Technology and Management at Sant\u0026rsquo;Anna School of Advanced Studies in Pisa and a guest scientist at the Chair of Ethics of AI and Neuroscience at the Institute for History and Ethics in Medicine at TU Munich. In his research, he addresses the ethical implications of cognitive integrations with different forms of technology: from generative AI to neurotechnologies. In his doctoral thesis he is analyzing how the extended mind thesis shapes the strength, scope and definition of neurorights.\n","permalink":"http://localhost:54178/past_talks/guidoc/","summary":"A philosophical foundation for the right to personal identity","title":"June 6: Guido Cassinadri (Sant'Anna)"},{"content":"Title Bioinspired Animal-Robot Interaction: A Relational Meta-Ethical Approach\nAbstract TBA\nAbout Marco Marco is a historian and philosopher of science and technology with broader interests in philosophy and its history. He is based at the TU Darmstadt (where he teaches as Privatdozent and is also P.I. of the project supported by the German Research Foundation (DFG) “Hybrid Systems, Bionics, and the Circulation of Morphological Knowledge in the Second Half of the 20th Century and the Early 21st Century”). Futhermore, he is member of the Junge Akademie | Mainz - the Young Academy of the Academy of Sciences and Literature, Mainz, fellow of the Johanna Quandt Young Academy as well as associate member at the Cluster of Excellence \u0026ldquo;Matters of Activity.\u0026rdquo;\n","permalink":"http://localhost:54178/past_talks/-marco/","summary":"Bioinspired Animal-Robot Interaction","title":"May 23: Professor Marco Tamborini (TU Darmstadt)"},{"content":"Title Algorithmic Recommendations: What’s the problem?\nAbstract We live in an increasingly datafied world, where information is abundant but difficult to navigate. In this context, we have become accustomed to interacting with recommender systems that personalise and structure the information we access in digital environments, which have emerged as one of the foremost applications of machine learning today. After introducing the basic features and techniques for algorithmic recommenders, I consider a currently dominant problem formulation for algorithmic recommendations in terms of a prediction task and critically assess its epistemological and normative assumptions.\nAbout Silvia Silvia is Senior Lecturer in Philosophy in the SPA Department and Egenis centre at the University of Exeter, and a Humboldt Fellow at the LMU Munich, based at the Munich Centre for Mathematical Philosophy. Previously, she was a Research Fellow at the Future of Humanity Institute (FHI) in the Faculty of Philosophy, University of Oxford and William Golding Junior Research Fellow at Brasenose College. Before that, she was a Postdoctoral Researcher at the Oxford Internet Institute, where she remains affiliated with the Governance of Emerging Technologies (GET) research programme. She completed a PhD in Philosophy at the London School of Economics and Political Science in 2018. Her interests lie primarily in epistemology, ethics, and intersections between the two, especially in connection with Artificial Intelligence. Her current research investigates epistemological and ethical issues relating to recommender systems.\n","permalink":"http://localhost:54178/past_talks/silvia/","summary":"Algorithmic recommendations: What\u0026rsquo;s the problem?","title":"May 9: Professor Silvia Milano (Exeter University)"},{"content":"Title Reasoning dependency\nAbstract I present a chapter from my book project on the nature and normative status of manipulation. The overall aim of the book is to defend the indifference account of manipulation in detail, arguing that manipulation is a type of indifference to reveal reasons to the target. In my talk, I will present the first chapter of the book, which describes the background view of human decision making against which I will develop the new account of manipulation. Specifically, this chapter argues that human reasoning—encompassing both the recognition of normative reasons and the capacity to act upon them—is inherently dependent on our social and environmental contexts. Rather than being solely an individual deliberative process, being \u0026ldquo;attuned to reason\u0026rdquo; emerges from a network of interactions that shape our values, choices, and actions. By examining everyday decisions and exploring the roles of both reflective and non-reflective processes, the chapter reveals how our ability to reason effectively is fundamentally extended through social scaffolding and environmental support. This perspective not only highlights the significance of our dependency for sound judgment but also sets the stage for a rethinking of manipulation. In particular, it shows how manipulation can disrupt the very conditions necessary for being attuned to reasons – not only by active, nefarious commission – as currently thought – but also through ‘mere indifference,’ which, I argue, is way more pernicious than currently recognized.\nAbout Michael Michael is Assistant Professor of Practical Philosophy at Delft University of Technology, where he explores pressing ethical questions in technology and society. His research spans metaethics, normative ethics, applied ethics, and the philosophy of technology, with a current focus on the ethics of influence and manipulation. Starting September 2025, he will lead an ERC-funded project on the nature and normative status of manipulation in the context of AI. He also leads several ongoing projects examining the ethics of manipulation and its implications for technological developments like advanced AI assistants, collaborating with engineers and policymakers to translate ethical insights into actionable frameworks for responsible research and innovation.\n","permalink":"http://localhost:54178/past_talks/michael/","summary":"Reasoning dependency","title":"April 11: Professor Michael Klenk (Delft University)"},{"content":"Title Automation and meaningful work\n❗ Note: This colloquium will take place at 13:00 instead of the usual 10:00. ❗\nAbstract In this talk, I argue that meaning in life has a more substantive subjective element than is commonly thought. This claim, if correct, has significant implications for debates on meaningful work. I argue that the meaningfulness of work can come apart from the extent to which workers obtain achievements at work. In doing so, I respond to Danaher and Nyholm’s (2021) recent argument that automation threatens meaningful work by leading to ‘achievement gaps’. Pace Danaher and Nyholm, I argue that even if automation threatens achievement, it does not necessarily follow that it threatens meaningful work.\nAbout Charlotte Charlotte Unruh is a Lecturer in Philosophy at the University of Southampton.\nPreviously, she was an Early Career Research Fellow at the Institute for Ethics in AI, Faculty of Philosophy, University of Oxford, and a Research Fellow at Corpus Christi College, Oxford. Prior to that, she was a Postdoctoral Researcher at the Chair of International Relations at the School of Social Science and Technology, Technical University of Munich, and a researcher at the TUM Institute for Ethics in AI.\nHer research focuses primarily on moral philosophy, with particular interests in the philosophy of harm, future generations, and the future of work.\n","permalink":"http://localhost:54178/past_talks/charlotte_u/","summary":"Automation and meaningful work (NB: to be held at 13:00, not 10:00)","title":"March 21: Professor Charlotte Unruh (University of Southhampton)"},{"content":"Title The Conversion Hypothesis: A Physical Approach to Qualia\nAbstract Through the advent of modern neurotechnology like brain-computer interfaces (abbr. BCIs) the data-based analysis of brain activity reaches new levels of precision, data fidelity, and accuracy both in brain imaging and scanning contexts. While these technological advances allow us to obtain ever-increasing volumes of data from the brain, the issue of interpreting this data remains. Correlation doesn´t equal causation; hence any observed brain activity might merely correlate temporarily with the subject´s experience of qualia instead of causing the very experience of qualia. Thus, the mysteries of qualia remain, and the incomprehensibly complex question of human experience continues: How – if at all – can our consciousness be explained? Possibly even more intriguing: Will the subjectivity of the first-person perspective eventually become accessible through technology, hence objectifying the subjective? Or is the sphere of qualia sealed and inherently protected from any kind of outward analysis?\nAbout Johannes Dr. phil. Johannes Lierfeld is a German author, screenwriter, and AI ethics researcher. He studied media studies, German studies, and phonetics in Marburg and Cologne, earning his doctorate in 2015 from the University of Cologne with a dissertation on terrorism, media, and cinema in the post-9/11 era.\nHe began his career as a screenwriter, contributing to SOKO Köln and developing series concepts for ProSieben and Sat.1. In 2009, he co-wrote the award-winning science fiction thriller 2012. Since 2014, he has focused on AI and ethics, publishing seven non-fiction books on the subject. His first novel, MEDICA (2017), was co-written with AI developer Scott Cote, followed by the non-fiction Artificial Superintelligence: Utopias, Dystopias, Disruptions (2018).\nLierfeld is currently pursuing a second doctorate at LMU Munich on the Conversion Hypothesis and brain-computer interfaces. He also works as a lecturer, speaker, and AI strategy consultant, including for Rotary International.\n","permalink":"http://localhost:54178/past_talks/johannes/","summary":"A physical approach to qualia","title":"March 07: Dr. Johannes Lierfeld (Munich)"},{"content":"Title Chapter from forthcoming book \u0026ldquo;Inbetweenism: why existing ethical positions are outdated when facing generative AI technology\u0026rdquo;\nAbstract TBA\nAbout Anna After taking postdoctoral positions as scientific researcher and coordinator at the Freiburg Center for Cognitive Science (2004-07), and at the Berlin School of Mind and Brain (2009-16), Anna was a Visiting Fellow at the Center for Cognitive Studies at Tufts University with Daniel Dennett (2018). She then founded the DenkWerkstatt Berlin, and now works as an independent, freelance philosopher.\nSince the autumn of 2020 Anna has been an associate researcher of the Cognition, Values, Behaviour (CVBE) research group at LMU-Munich, and, from March to June 2023, she was a Visiting Fellow in philosophy at UC Riverside, working with Eric Schwitzgebel.\n","permalink":"http://localhost:54178/past_talks/anna/","summary":"Chapter from forthcoming book \u0026lsquo;Inbetweenism\u0026rsquo;","title":"February 14: Dr. Anna Strasser (Denk Werkstatt Berlin)"},{"content":"Title Are algorithms more objective than humans? On the objectivity and epistemic authority of algorithms\nAbstract TBA\nAbout Carina Carina is Professor for the Ethics of Technology at Utrecht. She also teaches AI governance at Oxford University, and is affiliated with the Black Hole Initiative at Harvard University. Her main research focus is on autonomy and the ethics of automated decision-making, though she is also interested in the governance of AI more broadly.\n","permalink":"http://localhost:54178/past_talks/carina/","summary":"Are algorithms more objective than humans?","title":"February 7: Prof. Carina Prunkl (Utrecht/Oxford)"},{"content":"Title Valenced sentientism as a promising framework for addressing the ethical challenges raised by neurosurgery called hemispherotomy\nAbstract In the field of applied ethics, it is widely accepted that as soon as a creature is capable of having conscious experiences, it must be granted moral status and rights, such as the right not to die and not to suffer. However, it is currently a subject of intense debate whether the attribution of phenomenal consciousness is a necessary (see, e.g., Lin, 2020; Kriegel, 2019), sufficient (Van der Deijl, 2020), or neutral condition (Lee, 2019; Kammerer, 2019) for the attribution of moral status—specifically whether only sentience (valenced phenomenal experience) is morally significant. The aim of my presentation is to highlight the limitations of the phenomenal \u0026ldquo;sentientist\u0026rdquo; approach, which holds that welfare goods are eo ipso phenomenal goods, by relying on a concrete case study: the isolated hemisphere following hemispherotomy, which potentially presents itself as an island of negative phenomenology. I will argue that a more constrained theoretical framework, which is called \u0026ldquo;narrow sentientism\u0026rdquo; (see also Birch, 2024), is better suited to address the ethical debates posed by such challenging cases.\nAbout Charlotte Charlotte is a research associate at the University of Bonn, working at the Chair for Epistemology, Modern, and Contemporary Philosophy. She is also associated with the Center for Science and Thought in Bonn.\n","permalink":"http://localhost:54178/past_talks/charlotte/","summary":"Valenced sentietism","title":"January 31: Dr. Charlotte Gauvry (Bonn)"},{"content":"Title Can AI be a scientist?: A Conceptual Engineering Approach\nAbstract TBA\nAbout Guido Guido is Assistant Professor (TT) of Logic and AI at Vrije Universiteit Amsterdam. He received his (joint) PhD in philosophy (2020) from Ruhr University Bochum and École Normale Supérieure, Paris (Institut Jean Nicod). He works on the philosophy of language, the philosophy of psychology, and the philosophy of AI.\n","permalink":"http://localhost:54178/past_talks/guido/","summary":"Can AI be a scientist?: A Conceptual Engineering Approach","title":"January 24: Prof. Guido Löhr (Amsterdam)"},{"content":"Title The value of authenticity for meaning in life and implications for emerging technologies\nAbstract About Muriel Muriel is a postdoctoral researcher in the Digital Society Initiative and the Department of Philosophy of the University of Zurich. She is interested in the ethics of technology / AI, medical ethics (neuroethics in particular), philosophy of mind, meaning in life, philosophy of identity, authenticity, and genealogy. She previously worked at the Uehiro Center for Practical Ethics at Oxford, and before that, she received her PhD in philosophy at the University of Basel.\n","permalink":"http://localhost:54178/past_talks/muriel/","summary":"The value of authenticity for meaning in life and implications for emerging technologies","title":"November 29: Dr. Muriel Leuenberger (Zurich)"},{"content":"Title Consciousness, Machines, and Moral Status\nAbstract In light of recent breakneck pace in machine learning, questions about whether near-future artificial systems might be conscious and possess moral status are increasingly pressing. This paper argues that as matters stand these debates lack any clear criteria for resolution via the science of consciousness. Instead, insofar as they are settled at all, it is likely to be via shifts in public attitudes brought about by the increasingly close relationships between humans and AI users. Section 1 of the paper I briefly lays out the current state of the science of consciousness and its limitations insofar as these pertain to machine consciousness, and claims that there are no obvious consensus frameworks to inform public opinion on AI consciousness. Section 2 examines the rise of conversational chatbots or Social AI, and argues that in many cases, these elicit strong and sincere attributions of consciousness, mentality, and moral status from users, a trend likely to become more widespread. Section 3 presents an inconsistent triad for theories that attempt to link consciousness, behaviour, and moral status, noting that the trends in Social AI systems will likely make the inconsistency of these three premises more pressing. Finally, Section 4 presents some limited suggestions for how consciousness and AI research communities should respond to the gap between expert opinion and folk judgment.\nFull paper available here.\nAbout Henry Henry is Associate Teaching Professor of Philosophy at Cambridge University, as well as Associate Director of the Leverhulme Center for the Future of Intelligence. His research spans the philosophy of mind, the philosophy of artificial intelligence, and the ethics of artificial intelligence.\n","permalink":"http://localhost:54178/past_talks/henry/","summary":"Consciousness, Machines, and Moral Status","title":"November 22: Prof. Henry Shevlin (Cambridge)"},{"content":" Workshop Overview Deep learning model architectures are increasingly powerful, and are being used in increasingly many arenas of human life. To use them responsibly, we have to know something about how they work, and to what extent they process information like humans do. But that knowledge is hard to come by. In this workshop, we will discuss both how that knowledge might be gained, and how to use such deep learning models responsibly, given what we currently know.\nSchedule 10:00 - 10:40 Carlos Zednik. XAI and justification 10:40 - 11:20 Marius Bartmann and Bert Heinrichs. LLMs as semantic free riders 11:20 - 12:00 Celine Budding. Tacit knowledge in LLMs 12:00 - 1:00 PM Lunch 1:00 - 1:40 Charles Rathkopf. Hallucination and reliability 1:40 - 2:40 Roundtable discussion Join our workshop If you would like to attend, please write to Charles Rathkopf.\n","permalink":"http://localhost:54178/workshop/","summary":"with Carlos Zednik","title":"November 8: Jülich-Eindhoven Workshop on the Philosophy of Deep Learning"},{"content":"Title Robotics Affordance Mixtures and the \u0026lsquo;Kickable\u0026rsquo; Challenge to Anthropomorphism in Early Robot Ethics\nAbstract Although robots are often portrayed in the media as highly efficient and ubiquitous, the reality is far from this image. This is particularly true for non-industrial social robots. Recent multimodal AI systems seem to promise new advances in robotics that might begin to change this and bring robots into more and more environments. However, the normative conceptual space for how we evaluate the humans\u0026rsquo; treatment of robots is not well prepared for such an expansion. Most discussions are still motivated by graphic scenarios that depict humans mistreating robots (e.g. kicking a robot) and by anthropomorphism to describe human experience in HRI. As such, they result in concerns that the mistreatment of robots could have undesirable consequences or implications for human moral practices. Thus, they normatively problematize mistreatments. These approaches fail to consider broader, everyday interactions. These include the use of robots in everyday life, where robots come with unique affordance mixtures, don\u0026rsquo;t necessarily resemble/represent other beings, and are perceived directly (rather than fictionalized or anthropomorphized). As such, robots can be treated differently from people/animals. I introduce \u0026ldquo;robotic affordance mixtures\u0026rdquo; to describe the versatile capacities of robots. And I show how current approaches fail to account for the novel and versatile affordance mixtures that robots can offer; the novel and dynamic ways in which human-robot interactions can occur; the use of robots in many ways as products, thanks to affordance mixtures, while robots participate in the social realm. Then I apply ecological psychology and the conceptual tool of \u0026lsquo;sociomorphing\u0026rsquo; to emphasize how situation, context, dynamics, interaction and direct affordance perception matter in HRI, where robots play an active role rather than being passively anthropomorphized/fictionalized. With such tools, normative discussion can begin to distinguish between an affordance treatment of a robot from a mistreatment, an everyday use from an abuse. So, there is a need for more cognitive-scientific and conceptual insight into what I refer to as \u0026rsquo;early robot ethics\u0026rsquo;. In particular, into the indirect robot moral patiency debate, which has been stuck on graphic scenarios of mistreatment, and stuck on limited descriptive concepts like anthropomorphism. But now the debate needs to move away from such limited and inaccurate HRI formulations based on anthropomorphism—where interaction is understood passively and morally loaded descriptions of actions towards robots are taken for granted (\u0026lsquo;kicking a robot is mistreatment\u0026rsquo;). We need to challenge these assumptions and move towards more nuanced investigations of HRI by focusing on mundane everyday interactions—rather than graphic scenarios—with novel and versatile robotic affordance mixtures.\nAbout Arzu Arzu Formánek is a researcher at the Fraunhofer IPA-Institute for Manufacturing Engineering and Automation, a researcher at University of Stuttgart, and a PhD candidate at University of Vienna (Supervisors: Mark Coeckelbergh and Mark Bickhard. External Advisor: Sven Nyholm).\n","permalink":"http://localhost:54178/past_talks/arzu/","summary":"Robotics affordance mixtures and early robot ethics","title":"October 25: Arzu Formanek (Vienna)"},{"content":"Title Promoting Responsible Use of AI in African Healthcare: Strengthening Patient Moral Agency\nAbstract Machine learning technologies deployed in several sub-Saharan African countries to assist medical practitioners have shown how such technologies can significantly extend the reach of limited medical personnel and equipment resources. However, while I praise the efficiency of these technologies in carrying out medical diagnosis and treatment recommendations, I raise some critical concerns about the normative shift that may occur in their usage in the sub-Sahel. I argue that Overreliance on these technologies may threaten shared decision-making between patients and doctors. While shared decision-making is an integral component of patient-centred care in contemporary medicine that must be respected, from a phenomenological perspective, the stakes are higher for sub-Saharan Africans. For Africans, the threat of shared decision-making may negatively impact two significant aspects of the community: I) What constitutes moral agency – interpersonal relationships. I contend that overreliance on machine learning technology for clinical diagnosis and recommendations may diminish the value of interpersonal relationships between patients and doctors. II) Overreliance on these technologies may also negatively impact alternative forms of treatments and therapies. From an African Indigenous knowledge thought, on the one hand, I show how vital these normative values are in the making of a person and their moral agency within the sub-Saharan African value systems; on the other hand, I contend that interpersonal relationships contribute to curative medicine and patient holistic wellness. Finally, this paper seeks to make novel contributions to the development of value-sensitive healthcare/healthcare technology policies, guidelines, and regulations within sub-Saharan Africa and countries in the global South that share similar ethical/cultural worldviews.\nAbout Edmund Edmund is a researcher at the Centre for Africa-China Studies at the University of Johannesburg. Edmund is also a research fellow at the Centre for the Philosophy of Epidemiology, Medicine and Public Health – a joint centre between the University of Johannesburg and Durham University. He leads the project on African-Centred Healthcare Technologies. Finally, Edmund is a recipient of the University of Notre Dame – IBM Ethics Lab 2024 Grant on the project \u0026ldquo;Technology Transfer and Cultures in Africa: Large Language Model in Focus.\u0026rdquo;\n","permalink":"http://localhost:54178/past_talks/edmund/","summary":"Promoting Responsible Use of AI in African Healthcare","title":"October 18: Edmund Terem Ugar (Johannesburg)"},{"content":"Title AI, human-capacity habituation, and the deskilling problem\nAbstract AI tools replace or stand to replace human activity with non-human activity, via automated decision-making, recommender systems and content generation. The more AI replaces valuable human activity, the more it risks deskilling humans of their human capacities. While others have warned of moral deskilling caused by AI-warfare and social robotics, I argue that AI deskilling could encompass other human valuable capacities such as the epistemic, social, creative, physical and the capacity to will. On an Aristotelian view, deskilling of these capacities leads to capacity-impoverishment and ultimately to unflourishing lives.\nAI is uniquely positioned to accelerate deskilling, because it compromises the conditions necessary for cultivating capacities. Competently exercising human capacities is like cultivating a virtue. It requires habituation: practice, refinement and judgment. AI offers the possibility of replacing various human tasks through automation, via automated decision-making, recommendations, diagnostics, content creation, and robotics. While some replacements may not diminish skill levels, such as automating repetitive tasks, others could erode skills if they impede the conditions necessary for capacity development and habituation.\nTo determine when AI replacement is benign or ethically problematic, I offer an analytic framework for evaluating the goodness of AI tools and systems, distinguishing between two types of AI environments: environments that afford capacity habituation (e.g. practice; encountering challenge; discernment; flow), and capacity-hostile environments that narrow the ‘field of affordances’ necessary for capacity habituation. Notably, this helps identify AI environments that undermine the habituation of the capacity to will, which is a meta-capacity required for cultivating other capacities.\nFocusing on human capacities as an organizing idea is useful for AI ethics as it offers a vision of living well with AI through the cultivation and competent exercise of capacities; and a political morality that regulates against ‘capacity-hostile’ AI environments.\nAbout Avigail Avigail Ferdman is a Research Fellow at the Department of Humanities and Arts, Technion - Israel Institute of Technology. At the Technion she also leads the Embedded Ethics program.\n","permalink":"http://localhost:54178/past_talks/avigail/","summary":"AI, human-capacity habituation, and the deskilling problem","title":"July 12: Prof. Avigail Ferdman (Technion)"},{"content":"Title The ethics of cerebral organoid research: Searching for markers of consciousness\nAbstract Will (some) cerebral organoids acquire the capacity to develop consciousness, and if so what kind? In light of the advances of the technology concerns have been raised that over time these structures may develop mental capacities worthy of protection. This has spurred attempts to identify markers that warrant caution. But what markers should that be and can a marker-based approach account for the forms of consciousness cerebral organoids might develop? I argue that such deliberations benefit from a stronger focus on current laboratory practices that generate organoids as models for brain research.\nAbout Sarah Sarah is both a medical doctor and a philosopher. She is currently doing a postdoc at the university of Bonn.\n","permalink":"http://localhost:54178/past_talks/sarah/","summary":"The ethics of cerebral organoid research: Searching for markers of consciousness","title":"July 5: Dr. Sarah Diner (Bonn)"},{"content":"Title The Alignment Problem in Context\nAbstract A core challenge in the development of increasingly capable AI systems is to make them safe and reliable by ensuring their behaviour is consistent with human values. This challenge, known as the alignment problem, does not merely apply to hypothetical future AI systems that may pose catastrophic risks; it already applies to current systems, such as large language models, whose potential for harm is rapidly increasing. In this paper, I assess whether we are on track to solve the alignment problem for large language models, and what that means for the safety of future AI systems. I argue that existing strategies for alignment are insufficient, because large language models remain vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I offer an explanation of this lingering vulnerability on which it is not simply a contingent limitation of current language models, but has deep technical ties to a crucial aspect of what makes these models useful and versatile in the first place \u0026ndash; namely, their remarkable aptitude to learn \u0026ldquo;in context\u0026rdquo; directly from user instructions. It follows that the alignment problem is not only unsolved for current AI systems, but may be intrinsically difficult to solve without severely undermining their capabilities. Furthermore, this assessment raises concerns about the prospect of ensuring the safety of future and more capable AI systems.\nAbout Raphaël Raphaël is Assistant Professor in philosophy at Macquarie University in Sydney, Australia. He works mainly within the philosophy of artificial intelligence, cognitive science, and mind.\n","permalink":"http://localhost:54178/past_talks/rafael/","summary":"The Alignment Problem in Context","title":"June 21: Prof. Raphaël Millière (Macquarie)"},{"content":"Title What gives the Ethics of AI its Bad Name?\nAbstract I will consider what exactly the value of the ethics of AI is in the current AI regulatory context amidst different calls for keeping AI innovation ‘responsible’ and formulating some kind of international regulation of AI. I will make suggestions on how AI ethics contributes to upholding human rights law, standards, and principles; and also, on how to delineate, but at the same time connect it to AI safety conversations. I will conclude with suggestions on how to ensure AI ethics is an enabler of sound and trustworthy AI innovation.\nAbout Emma Emma Ruttkamp-Bloem is Professor of Philosophy at the University of Pretoria.\n","permalink":"http://localhost:54178/past_talks/emma/","summary":"What gives the Ethics of AI its Bad Name?","title":"May 17: Prof. Emma Ruttkamp-Bloem (Pretoria)"},{"content":"Title Russell and Floridi on Artificial Intelligence and Agency\nAbstract Are (some) AI technologies agents and if so, in what sense(s)? And why consider this topic? One interesting similarity between Stuart Russell\u0026rsquo;s discussion of AI in his book Human Compatible (2019) and Luciano Floridi\u0026rsquo;s discussion of AI in his book Ethics of Artificial Intelligence (2023) is that both of them define artificial intelligence in terms of agency. In my presentation, I will first motivate why it is interesting to reflect on whether at least some AI technologies are agents and then compare and contrast the agency-based definitions of AI in the Russell and Floridi\u0026rsquo;s respective books. I will argue that their views can be combined in interesting ways but that it is best to define AI in a way that leaves it open whether AI technologies are agents in any significant sense.\nAbout Sven Sven Nyholm is Professor of Ethics of Artificial Intelligence at the LMU.\n","permalink":"http://localhost:54178/past_talks/sven/","summary":"Russell and Floridi on Artificial Intelligence and Agency","title":"April 19: Prof. Sven Nyholm (LMU)"},{"content":"Title The Reasons of AI Systems (co-authored with Kevin Baum, Maximilian Schlüter, Timo Speith)\nAbstract This paper connects the fields of philosohy of action and of explainable artificial intelligence (AI). We investigate whether it can ever be appropriate to explain the outputs of AI systems by appeal to practical reasons and reasoning of these systems. We argue that this can indeed be fitting. To this end, we first present an argument that starts from the premise that we use AI systems because they work so well for us and then defend our claim against four objections.\nAbout Eva Eva Schmidt is Professor of theoretical philosophy at the TU Dortmund.\n","permalink":"http://localhost:54178/past_talks/eva/","summary":"The Reasons of AI Systems","title":"April 12: Prof. Eva Schmidt (Dortmund)"},{"content":"Title The human behind human-centered AI\nAbstract In philosophy and other disciplines, the question what distinguishes humans from animals has been debated for centuries. However, current technological advancements in the field of artificial intelligence raise concerns about the distinction between humans and machines. What is therefore meant if legislators or AI ethicists claim that the development and deployment of AI systems are supposed to be human-centered? Is this human-centricity actually (just) a synonym for value-centered design? And can this \u0026ldquo;humanness\u0026rdquo; be translated into code, standards or norms?\nAbout Julia Julie Mönig is a Postdoc Research Associate at the CST, Project manager of the subproject philosophy in the AI.NRW flagship project \u0026ldquo;Certified AI\u0026rdquo;, and Principle Investigator of the project \u0026ldquo;Rethinking Privacy after this Pandemic.\u0026rdquo;\n","permalink":"http://localhost:54178/past_talks/julia/","summary":"The human behind human-centered AI","title":"March 15: Dr. Julia Mönig (Bonn)"},{"content":"Title Bad Habits: The Morality of Smartphone Use in Leisure Time.\nAbstract This paper argues that the habit of frequently using a smartphone is highly unde- sirable from a moral perspective. It is highly morally undesirable because by having the habit, the probability significantly increases that one will perform various morally impermissible actions. For example, by having the habit, the probability that one will regularly text while driving rises significantly. The practice of regularly texting while driving is pro tanto morally impermissible. Hence having the habit of frequently using a smartphone significantly increases the probability that one will do something morally impermissible. The fact that having the habit significantly increases the probability that one will do something morally impermissible adds weight to the proposition that the habit is morally undesirable. I add further weight to the proposition that the habit is morally undesirable by arguing that the habit of frequently using a smartphone significantly increases the probability that one will engage in smartphone use while in any critical situation, not just while driving; that one will regularly engage in disruptive smartphone use in inter- personal contexts (thereby courting the risk of harming certain interests others have in being known and loved); and that one will regularly engage in smartphone use in such a way that one’s own autonomy and perhaps also authenticity are degraded. In virtue of the habit being very morally undesirable, I argue that we are morally required to take reasonable steps in order to avoid it. I argue that included in these steps is generally refraining from smartphone usage while at leisure. For by using a smartphone at leisure, we court the risk of developing a habit that is very morally undesirable. Thus, my argument suggests that using a smartphone while at leisure is often all-things-considered morally impermissible.\nAbout Sam Sam is a PhD student at the University of Georgia.\nTime change! Since Sam is in the US, this talk is scheduled for 14:00 CET, rather than the usual 10:00 CET.\n","permalink":"http://localhost:54178/past_talks/sam/","summary":"The morality of smartphone use.","title":"March 1: Dr. Sam Bennett (U. Georgia)"},{"content":"Topic What should AI ethics try to achieve?\nAbout Vincent Vincent Müller is the Humboldt Professor for the Philosophy of Artificial Intelligence at the Univeristy of Erlangen.\n","permalink":"http://localhost:54178/past_talks/vincent/","summary":"Title: What should AI ethics try to achieve?","title":"January 19: Prof. Vincent Müller (Erlangen)"},{"content":"November 8, 2024 Workshop Overview Deep learning model architectures are increasingly powerful, and are being used in increasingly many arenas of human life. To use them responsibly, we have to know something about how they work, and to what extent they process information like humans do. But that knowledge is hard to come by. In this workshop, we will discuss both how that knowledge might be gained, and how to use such deep learning models responsibly, given what we currently know.\nSchedule 10:00 - 10:40 Carlos Zednik. XAI and justification 10:40 - 11:20 Marius Bartmann and Bert Heinrichs. LLMs as semantic free riders 11:20 - 12:00 Celine Budding. Tacit knowledge in LLMs 12:00 - 1:00 PM Lunch 1:00 - 1:40 Charles Rathkopf. Hallucination and reliability 1:40 - 2:40 Roundtable discussion Join our workshop If you would like to attend, please write to Charles Rathkopf.\n","permalink":"http://localhost:54178/workshop/","summary":"\u003ch2 id=\"november-8-2024\"\u003eNovember 8, 2024\u003c/h2\u003e\n\u003ch2 id=\"workshop-overview\"\u003eWorkshop Overview\u003c/h2\u003e\n\u003cp\u003eDeep learning model architectures are increasingly powerful, and are being used in increasingly many arenas of human life. To use them responsibly, we have to know something about how they work, and to what extent they process information like humans do. But that knowledge is hard to come by. In this workshop, we will discuss both how that knowledge might be gained, and how to use such deep learning models responsibly, given what we currently know.\u003c/p\u003e","title":"Jülich-Eindhoven Workshop on the Philosophy of Deep Learning"},{"content":"Title What is social interaction?\nAbout Tobias Tobias Schlicht is Professor of Philosophy at Ruhr-Universität Bochum. His is interested in questions regarding cognition and consciousness, has written three books on the mind-body problem and on social cognition, edited various collections and special issues on topics in cognitive science such as mental representation, and published papers on topics in this area.\nAbstract Discussions on social cognition are typically focused on the various strategies we can use to understand each other. Sometimes we can simply perceive another\u0026rsquo;s emotion in their facial and bodily expression, sometimes we rely on a body of knowledge to attribute beliefs or desires, and at other times we put ourselves in their mental shoes. Direct face-to-face interaction, in contrast to mere observation, has been recognized to be highly relevant for the application of such strategies and the kind of understanding of the other’s mindset. But it is rarely if never specified what interaction amounts to and what makes an interaction a social interaction rather than an engagement with a physical object. This question becomes more relevant in light of technological progress in social robotics where the question is whether we should take the intentional or design stance towards such AI systems which may either remain physical tools or turn into colleagues and friends. It is also relevant when we consider the ways humans interact with each other via social media or video calls, for example, since many features of ‚typical\u0026rsquo; social interactions may be absent, such as sharing a common situation or embodiment. We may mistakenly identify bots as other agents, for example. I will introduce a strategy to approach the question, which is work in progress.\n","permalink":"http://localhost:54178/past_talks/tobias/","summary":"Title: What is social interaction?","title":"November 17: Prof. Tobias Schlicht (Bochum)"},{"content":"Title Bewertung von KI-Feedback Systeme in der Lehre\nAbout Sebastian Sebastian Weydner-Volkman is Professor for the Ethics of Digital Methods and Technologies at the University of Bochum.\n","permalink":"http://localhost:54178/past_talks/sebastian/","summary":"Bewertung von KI-Feedback Systeme in der Lehre","title":"October 27: Prof. Sebastian Weydner-Volkman (Bochum)"},{"content":"Title Dual-use and artificial intelligence\nAbout Martin Dr. Martin Hähnel is a researcher in applied philosophy at the University of Bremen.\n","permalink":"http://localhost:54178/past_talks/martin/","summary":"Dual-use and artificial intelligence","title":"October 20: Dr. Martin Hähnel (Bremen)"},{"content":"Title No well-being for robots (and hence no robot rights)\nAbout Peter Peter Königs is Professor of Philosophy at the TU Dortmund.\n","permalink":"http://localhost:54178/past_talks/peter/","summary":"No well-being for robots (and hence no robot rights)","title":"June 2: Prof. Peter Königs (Dortmund)"},{"content":"Title Digitale Transformation im Gesundheitswesen\nAbout Janina Janina is honorary professor (Ethik der Technik und ihrer sozialen Kontexte) at the Hochschule Bonn-Rhein-Sieg, University for Applied Studies, Centre for Ethics and Responsibility (ZEV). Janina is also on the ethics staff (Stabstelle Ethik) at the Liebenau Foundation.\n","permalink":"http://localhost:54178/past_talks/janina/","summary":"Digitale Transformation im Gesundheitswesen","title":"May 26: Prof. Janina Loh (Bonn-Rhein-Sieg)"},{"content":"We meet on Fridays at 10:00 CET.\nIf you would like to join the seminar, please write to Markus Rüther and ask to be added to the mailing list. You will then receive emails with the link to our Zoom invite each week.\n","permalink":"http://localhost:54178/contact/","summary":"\u003cp\u003eWe meet on Fridays at 10:00 CET.\u003c/p\u003e\n\u003cp\u003eIf you would like to join the seminar, please write to \u003ca href=\"mailto:m.ruether@fz-juelich.de\"\u003eMarkus Rüther\u003c/a\u003e and ask to be added to the mailing list. You will then receive emails with the link to our Zoom invite each week.\u003c/p\u003e","title":"Join our virtual meetings"}]