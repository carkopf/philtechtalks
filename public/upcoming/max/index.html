<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>March 10: Max Hellrigel-Holderbaum (Erlangen) | Phil.Tech.Talks</title>
<meta name=keywords content><meta name=description content="AI Safety Assessment"><meta name=author content><link rel=canonical href=https://philtechtalks.netlify.app/upcoming/max/><meta name=google-site-verification content="G-ENE3M094RN"><link crossorigin=anonymous href=/assets/css/stylesheet.9c8530f2c62fb1bbcb8855a8f50041f6c4f663711d26b31c460c62ec78068a0a.css integrity="sha256-nIUw8sYvsbvLiFWo9QBB9sT2Y3EdJrMcRgxi7HgGigo=" rel="preload stylesheet" as=style><link rel=icon href=https://philtechtalks.netlify.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://philtechtalks.netlify.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://philtechtalks.netlify.app/favicon-32x32.png><link rel=apple-touch-icon href=https://philtechtalks.netlify.app/apple-touch-icon.png><link rel=mask-icon href=https://philtechtalks.netlify.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://philtechtalks.netlify.app/upcoming/max/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ENE3M094RN"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ENE3M094RN")}</script><meta property="og:title" content="March 10: Max Hellrigel-Holderbaum (Erlangen)"><meta property="og:description" content="AI Safety Assessment"><meta property="og:type" content="article"><meta property="og:url" content="https://philtechtalks.netlify.app/upcoming/max/"><meta property="article:section" content="upcoming"><meta property="article:published_time" content="2026-03-10T10:00:10+01:00"><meta property="article:modified_time" content="2026-03-10T10:00:10+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="March 10: Max Hellrigel-Holderbaum (Erlangen)"><meta name=twitter:description content="AI Safety Assessment"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Upcoming","item":"https://philtechtalks.netlify.app/upcoming/"},{"@type":"ListItem","position":2,"name":"March 10: Max Hellrigel-Holderbaum (Erlangen)","item":"https://philtechtalks.netlify.app/upcoming/max/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"March 10: Max Hellrigel-Holderbaum (Erlangen)","name":"March 10: Max Hellrigel-Holderbaum (Erlangen)","description":"AI Safety Assessment","keywords":[],"articleBody":"Title The Streetlight Effect in AI Safety Assessments\nAbstract As AI systems advance in capabilities, measuring their safety and alignment to human values is becoming paramount. A fast-growing field of AI research is devoted to developing such assessments. However, most current advances therein may be ill-suited for assessing AI systems across real-world deployments.\nStandard methods prompt large language models (LLMs) in a questionnaire-style to describe their values or behavior in hypothetical scenarios. By focusing on unaugmented LLMs, they fall short of evaluating AI agents, which could actually perform relevant behaviors, hence posing much greater risks. LLMs‚Äô engagement with scenarios described by questionnaire-style prompts differs starkly from that of agents based on the same LLMs, as reflected in divergences in the inputs, possible actions, environmental interactions, and likely internal processing between both. As such, LLMs‚Äô responses to scenario descriptions are unlikely to be representative of the corresponding LLM agents‚Äô behavior.\nWe further contend that such assessments make strong assumptions concerning the ability and tendency of LLMs to report accurately about their counterfactual behavior. This makes them inadequate to assess risks from AI systems in real-world contexts as they lack the necessary construct validity. We then argue that a structurally identical issue holds for current AI alignment approaches. Lastly, we discuss improving safety assessments and alignment training by taking these shortcomings to heart.\nAbout Max Max is a PhD candidate at the Centre for Philosophy and AI Research (PAIR) at FAU. His research focuses on risks from AI systems, specifically on concepts that are often invoked but rarely scrutinized in this context (such as goals, agency, manipulation, and planning). Recently, he has also explored topics at the intersection of philosophy of AI and philosophy of science. Before joining PAIR, he studied philosophy, political science, and neuroscience at Goethe University Frankfurt and Humboldt University of Berlin.\n","wordCount":"303","inLanguage":"en","datePublished":"2026-03-10T10:00:10+01:00","dateModified":"2026-03-10T10:00:10+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://philtechtalks.netlify.app/upcoming/max/"},"publisher":{"@type":"Organization","name":"Phil.Tech.Talks","logo":{"@type":"ImageObject","url":"https://philtechtalks.netlify.app/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://philtechtalks.netlify.app/ accesskey=h title="Phil.Tech.Talks (Alt + H)">Phil.Tech.Talks</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://philtechtalks.netlify.app/past_talks/ title="Past talks"><span>Past talks</span></a></li><li><a href=https://philtechtalks.netlify.app/workshop/ title=Workshop><span>Workshop</span></a></li><li><a href=https://philtechtalks.netlify.app/contact/ title="Join in"><span>Join in</span></a></li><li><a href=https://philtechtalks.netlify.app/search/ title="üîç (Alt + /)" accesskey=/><span>üîç</span></a></li></ul><div class=juelich-logo><a href=https://www.fz-juelich.de/en target=_blank title="Forschungszentrum J√ºlich"><img src=/juelich-logo.svg alt="Forschungszentrum J√ºlich" height=140></a></div></nav></header><style>.juelich-logo{display:flex;align-items:center;margin-left:auto;padding-left:20px}.juelich-logo img{height:140px;width:auto;opacity:.85;transition:opacity .2s,transform .2s ease}.juelich-logo img:hover{opacity:1;transform:scale(1.05)}@media screen and (max-width:768px){.juelich-logo{display:none}}</style><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">March 10: Max Hellrigel-Holderbaum (Erlangen)</h1></header><div class=post-content><h4 id=title>Title<a hidden class=anchor aria-hidden=true href=#title>#</a></h4><p>The Streetlight Effect in AI Safety Assessments</p><h4 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h4><p>As AI systems advance in capabilities, measuring their safety and alignment to human values is becoming paramount. A fast-growing field of AI research is devoted to developing such assessments. However, most current advances therein may be ill-suited for assessing AI systems across real-world deployments.</p><p>Standard methods prompt large language models (LLMs) in a questionnaire-style to describe their values or behavior in hypothetical scenarios. By focusing on unaugmented LLMs, they fall short of evaluating AI agents, which could actually perform relevant behaviors, hence posing much greater risks. LLMs&rsquo; engagement with scenarios described by questionnaire-style prompts differs starkly from that of agents based on the same LLMs, as reflected in divergences in the inputs, possible actions, environmental interactions, and likely internal processing between both. As such, LLMs&rsquo; responses to scenario descriptions are unlikely to be representative of the corresponding LLM agents&rsquo; behavior.</p><p>We further contend that such assessments make strong assumptions concerning the ability and tendency of LLMs to report accurately about their counterfactual behavior. This makes them inadequate to assess risks from AI systems in real-world contexts as they lack the necessary construct validity. We then argue that a structurally identical issue holds for current AI alignment approaches. Lastly, we discuss improving safety assessments and alignment training by taking these shortcomings to heart.</p><h4 id=about-max>About <a href>Max</a><a hidden class=anchor aria-hidden=true href=#about-max>#</a></h4><p>Max is a PhD candidate at the Centre for Philosophy and AI Research (PAIR) at FAU. His research focuses on risks from AI systems, specifically on concepts that are often invoked but rarely scrutinized in this context (such as goals, agency, manipulation, and planning). Recently, he has also explored topics at the intersection of philosophy of AI and philosophy of science. Before joining PAIR, he studied philosophy, political science, and neuroscience at Goethe University Frankfurt and Humboldt University of Berlin.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>Site maintained by <a href=mailto:charles.rathkopf@gmail.com>Charles Rathkopf</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>