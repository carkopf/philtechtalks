<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>October 10: Professor Kate Vredenburgh (London School of Economics) | Phil.Tech.Talks</title>
<meta name=keywords content><meta name=description content="What should we explain with explainable AI?"><meta name=author content><link rel=canonical href=https://philtechtalks.netlify.app/past_talks/kate/><meta name=google-site-verification content="G-ENE3M094RN"><link crossorigin=anonymous href=/assets/css/stylesheet.9c8530f2c62fb1bbcb8855a8f50041f6c4f663711d26b31c460c62ec78068a0a.css integrity="sha256-nIUw8sYvsbvLiFWo9QBB9sT2Y3EdJrMcRgxi7HgGigo=" rel="preload stylesheet" as=style><link rel=icon href=https://philtechtalks.netlify.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://philtechtalks.netlify.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://philtechtalks.netlify.app/favicon-32x32.png><link rel=apple-touch-icon href=https://philtechtalks.netlify.app/apple-touch-icon.png><link rel=mask-icon href=https://philtechtalks.netlify.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://philtechtalks.netlify.app/past_talks/kate/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ENE3M094RN"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ENE3M094RN")}</script><meta property="og:title" content="October 10: Professor Kate Vredenburgh (London School of Economics)"><meta property="og:description" content="What should we explain with explainable AI?"><meta property="og:type" content="article"><meta property="og:url" content="https://philtechtalks.netlify.app/past_talks/kate/"><meta property="article:section" content="past_talks"><meta property="article:published_time" content="2025-10-10T10:00:10+01:00"><meta property="article:modified_time" content="2025-10-10T10:00:10+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="October 10: Professor Kate Vredenburgh (London School of Economics)"><meta name=twitter:description content="What should we explain with explainable AI?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Past_talks","item":"https://philtechtalks.netlify.app/past_talks/"},{"@type":"ListItem","position":2,"name":"October 10: Professor Kate Vredenburgh (London School of Economics)","item":"https://philtechtalks.netlify.app/past_talks/kate/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"October 10: Professor Kate Vredenburgh (London School of Economics)","name":"October 10: Professor Kate Vredenburgh (London School of Economics)","description":"What should we explain with explainable AI?","keywords":[],"articleBody":"Title What should we explain with explainable AI?\nAbstract Explaining opaque models is important to serve various practical or moral ends. XAI ‚Äì both the practice and the philosophy thereof ‚Äì seems to be out of step with best practice in science communication and evidence-based policy. While these fields aim to explain some aspect of the world using scientific models, debates on XAI assume that the model itself is, first and foremost, the correct target of the explanation. We examine that assumption, using the example of explanations of lending decisions. Consumer credit is a fruitful case study for the moral importance of explanation because has one of the best regulated explanatory requirements across domains, and the ways in which those explanations fall short are instructive. We argue that, in the domain of consumer lending, XAI is correct to focus on explanations of how the model works, due to the instrumental value of credit. The argument for this conclusion sets up an overlooked issue about its right to explanation: its distributional consequences. The right to explanation seems to be objectionably inegalitarian, at least in the case of consumer credit.\nAbout Kate Kate is Associate Professor in the Department of Philosophy, Logic and Scientific Method at the London School of Economics. She works on questions across the philosophy of social science, political philosophy, and the philosophy of AI.\nFrom 2024-2028, she will be investigating AI, worker autonomy, and the future of work with funding from a UKRI Future Leaders Fellowship Grant.\n","wordCount":"249","inLanguage":"en","datePublished":"2025-10-10T10:00:10+01:00","dateModified":"2025-10-10T10:00:10+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://philtechtalks.netlify.app/past_talks/kate/"},"publisher":{"@type":"Organization","name":"Phil.Tech.Talks","logo":{"@type":"ImageObject","url":"https://philtechtalks.netlify.app/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://philtechtalks.netlify.app/ accesskey=h title="Phil.Tech.Talks (Alt + H)">Phil.Tech.Talks</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://philtechtalks.netlify.app/past_talks/ title="Past talks"><span>Past talks</span></a></li><li><a href=https://philtechtalks.netlify.app/workshop/ title=Workshop><span>Workshop</span></a></li><li><a href=https://philtechtalks.netlify.app/contact/ title="Join in"><span>Join in</span></a></li><li><a href=https://philtechtalks.netlify.app/search/ title="üîç (Alt + /)" accesskey=/><span>üîç</span></a></li></ul><div class=juelich-logo><a href=https://www.fz-juelich.de/en target=_blank title="Forschungszentrum J√ºlich"><img src=/juelich-logo.svg alt="Forschungszentrum J√ºlich" height=140></a></div></nav></header><style>.juelich-logo{display:flex;align-items:center;margin-left:auto;padding-left:20px}.juelich-logo img{height:140px;width:auto;opacity:.85;transition:opacity .2s,transform .2s ease}.juelich-logo img:hover{opacity:1;transform:scale(1.05)}@media screen and (max-width:768px){.juelich-logo{display:none}}</style><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">October 10: Professor Kate Vredenburgh (London School of Economics)</h1></header><div class=post-content><h4 id=title>Title<a hidden class=anchor aria-hidden=true href=#title>#</a></h4><p>What should we explain with explainable AI?</p><h4 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h4><p>Explaining opaque models is important to serve various practical or moral ends. XAI ‚Äì both the practice and the philosophy thereof ‚Äì seems to be out of step with best practice in science communication and evidence-based policy. While these fields aim to explain some aspect of the world using scientific models, debates on XAI assume that the model itself is, first and foremost, the correct target of the explanation. We examine that assumption, using the example of explanations of lending decisions. Consumer credit is a fruitful case study for the moral importance of explanation because has one of the best regulated explanatory requirements across domains, and the ways in which those explanations fall short are instructive. We argue that, in the domain of consumer lending, XAI is correct to focus on explanations of how the model works, due to the instrumental value of credit. The argument for this conclusion sets up an overlooked issue about its right to explanation: its distributional consequences. The right to explanation seems to be objectionably inegalitarian, at least in the case of consumer credit.</p><h4 id=about-katehttpskatevredenburghcom>About <a href=https://katevredenburgh.com>Kate</a><a hidden class=anchor aria-hidden=true href=#about-katehttpskatevredenburghcom>#</a></h4><p>Kate is Associate Professor in the Department of Philosophy, Logic and Scientific Method at the London School of Economics. She works on questions across the philosophy of social science, political philosophy, and the philosophy of AI.</p><p>From 2024-2028, she will be investigating AI, worker autonomy, and the future of work with funding from a UKRI Future Leaders Fellowship Grant.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>Site maintained by <a href=mailto:charles.rathkopf@gmail.com>Charles Rathkopf</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>